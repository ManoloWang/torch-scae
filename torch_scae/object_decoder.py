# coding=utf-8
# Copyright 2020 The Google Research Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Capsule layer."""
from collections import namedtuple

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from monty.collections import AttrDict
from torch.distributions import Bernoulli, LogisticNormal, Normal

from torch_scae import cv_ops, math_ops
from torch_scae import nn_ext
from torch_scae.general_utils import prod
from torch_scae.math_ops import l2_loss


class CapsuleLayer(nn.Module):
    """Implementation of a capsule layer."""

    # number of parameters needed to parametrize linear transformations.
    _n_transform_params = 6

    def __init__(self,
                 n_caps,
                 dim_feature,
                 n_votes,
                 n_caps_params,
                 hidden_sizes=(128,),
                 learn_vote_scale=False,
                 deformations=True,
                 noise_type=None,
                 noise_scale=0.,
                 similarity_transform=True,
                 caps_dropout_rate=0.0):
        """Builds the module.

        Args:
          n_caps: int, number of capsules.
          n_caps_params: int, number of capsule parameters
          hidden_sizes: int or sequence of ints, number of hidden units for an MLP
            which predicts capsule params from the input encoding.
          n_caps_dims: int, number of capsule coordinates.
          n_votes: int, number of votes generated by each capsule.
          learn_vote_scale: bool, learns input-dependent scale for each
            capsules' votes.
          deformations: bool, allows input-dependent deformations of capsule-part
            relationships.
          noise_type: 'normal', 'logistic' or None; noise type injected into
            presence logits.
          noise_scale: float >= 0. scale parameters for the noise.
          similarity_transform: boolean; uses similarity transforms if True.
          caps_dropout_rate: float in [0, 1].
        """
        super().__init__()

        self._n_caps = n_caps  # O
        self._dim_feature = dim_feature  # F
        self._hidden_sizes = list(hidden_sizes)  # [H_i, ...]
        self._n_caps_params = n_caps_params  # P

        # self._n_caps_dims = n_caps_dims

        self._n_votes = n_votes
        self._learn_vote_scale = learn_vote_scale
        self._deformations = deformations
        self._noise_type = noise_type
        self._noise_scale = noise_scale

        self._similarity_transform = similarity_transform
        self._caps_dropout_rate = caps_dropout_rate

        self.build()

    def build(self):
        # Use separate parameters to do predictions for different capsules.
        sizes = [self._dim_feature] + self._hidden_sizes + [self._n_caps_params]
        self.mlps = nn.ModuleList([
            nn_ext.MLP(sizes=sizes)
            for _ in range(self._n_caps)
        ])

        self.output_shapes = (
            [self._n_votes, self._n_transform_params],  # CPR_dynamic
            [1, self._n_transform_params],  # CCR
            [1],  # per-capsule presence
            [self._n_votes],  # per-vote-presence
            [self._n_votes],  # per-vote scale
        )
        self.splits = [prod(i) for i in self.output_shapes]
        self.n_outputs = sum(self.splits)  # A

        # we don't use bias in the output layer in order to separate the static
        # and dynamic parts of the CPR
        sizes = [self._n_caps_params + 1] + self._hidden_sizes + [self.n_outputs]
        self.caps_mlps = nn.ModuleList([
            nn_ext.MLP(sizes=sizes, bias=False)
            for _ in range(self._n_caps)
        ])

        self.caps_biases = [nn.Parameter(torch.zeros(1, self._n_caps, *shape),
                                         requires_grad=True)
                            for shape in self.output_shapes[1:]]

        self.cpr_static = nn.Parameter(
            torch.zeros([1, self._n_caps, self._n_votes, self._n_transform_params]),
            requires_grad=True
        )

    def forward(self, features, parent_transform=None, parent_presence=None):
        """Builds the module.

        Args:
          features: Tensor of encodings of shape [B, O, F].
          parent_transform: Tuple of (matrix, vector).
          parent_presence: pass

        Returns:
          A bunch of stuff.
        """
        batch_size = features.shape[0]  # B

        # Predict capsule and additional params from the input encoding.
        # [B, O, P]

        caps_features = features.unbind(1)  # [(B, F)] * O
        caps_params_list = [self.mlps[i](caps_features[i])
                            for i in range(self._n_caps)]  # [(B, P)] * O
        raw_caps_params = torch.stack(caps_params_list, 1)  # (B, O, P)

        if self._caps_dropout_rate == 0.0:
            caps_exist = torch.ones(batch_size, self._n_caps, 1)  # (B, O, 1)
        else:
            pmf = Bernoulli(1. - self._caps_dropout_rate)
            caps_exist = pmf.sample((batch_size, self._n_caps, 1))  # (B, O, 1)

        caps_params = torch.cat([raw_caps_params, caps_exist], -1)  # (B, O, P+1)

        caps_eparams_list = caps_params.unbind(1)  # [(B, P+1)] * O
        all_params_list = [self.caps_mlps[i](caps_eparams_list[i])
                           for i in range(self._n_caps)]  # [(B, A)] * O
        all_params = torch.stack(all_params_list, 1)  # (B, O, A)
        all_params_split_list = torch.split(all_params, self.splits, -1)
        result = [t.view(batch_size, self._n_caps, *s)
                  for (t, s) in zip(all_params_split_list, self.output_shapes)]

        cpr_dynamic = result[0]

        # add bias to all remaining outputs
        ccr, pres_logit_per_caps, pres_logit_per_vote, scale_per_vote = [
            t + bias
            for (t, bias) in zip(result[1:], self.caps_biases)
        ]

        if self._caps_dropout_rate > 0.0:
            pres_logit_per_caps += math_ops.log_safe(caps_exist)

        def add_noise(tensor):
            """Adds noise to tensors."""
            if self._noise_type == 'uniform':
                noise = (torch.rand(tensor.shape) - 0.5) * self._noise_scale
            elif self._noise_type == 'logistic':
                pdf = LogisticNormal(0., self._noise_scale)
                noise = pdf.sample(tensor.shape)
            elif not self._noise_type:
                noise = 0.
            else:
                raise ValueError(f'Invalid noise type: {self._noise_type}')
            return tensor + noise

        pres_logit_per_caps = add_noise(pres_logit_per_caps)
        pres_logit_per_vote = add_noise(pres_logit_per_vote)

        # this is for hierarchical
        if parent_transform is None:
            ccr = self._make_transform(ccr)
        else:
            ccr = parent_transform

        if not self._deformations:
            cpr_dynamic = torch.zeros_like(cpr_dynamic)

        cpr = self._make_transform(cpr_dynamic + self.cpr_static)
        ccr_per_vote = ccr.repeat(1, 1, self._n_votes, 1, 1)
        votes = torch.matmul(ccr_per_vote, cpr)

        if parent_presence is not None:
            pres_per_caps = parent_presence
        else:
            pres_per_caps = torch.sigmoid(pres_logit_per_caps)

        pres_per_vote = pres_per_caps * torch.sigmoid(pres_logit_per_vote)

        if self._learn_vote_scale:
            # for numerical stability
            scale_per_vote = F.softplus(scale_per_vote + .5) + 1e-2
        else:
            scale_per_vote = torch.zeros_like(scale_per_vote) + 1.

        return AttrDict(
            vote=votes,
            scale=scale_per_vote,
            vote_presence=pres_per_vote,
            pres_logit_per_caps=pres_logit_per_caps,
            pres_logit_per_vote=pres_logit_per_vote,
            dynamic_weights_l2=l2_loss(cpr_dynamic) / batch_size,
            raw_caps_params=raw_caps_params,
            raw_caps_features=features,
        )

    def _make_transform(self, params):
        return cv_ops.geometric_transform(params, self._similarity_transform,
                                          nonlinear=True, as_matrix=True)


class CapsuleLikelihood(nn.Module):
    """Capsule voting mechanism."""
    Result = namedtuple(
        'CapsuleLikelihoodResult',  # pylint:disable=invalid-name
        ('log_prob',
         'vote_presence',
         'winner',
         'winner_pres',
         'is_from_capsule',
         'mixing_logits',
         'mixing_log_prob',
         'soft_winner',
         'soft_winner_pres',
         'posterior_mixing_probs')
    )

    def __init__(self, votes, scales, vote_presence_prob):
        super().__init__()
        self._n_votes = 1
        self._n_caps = int(votes.shape[1])  # O
        self._votes = votes  # (B, O, M, P)
        self._scales = scales  # (B, O, M)
        self._vote_presence_prob = vote_presence_prob  # (B, O, M)

        self.dummy_vote = nn.Parameter(
            torch.zeros(1, 1, *votes.shape[2:]),
            requires_grad=True
        )

    def _get_pdf(self, votes, scales):
        return Normal(votes, scales)

    def log_prob(self, x, presence=None):
        return self(x, presence).log_prob

    def explain(self, x, presence=None):
        return self(x, presence).winner

    def forward(self, x, presence=None):  # (B, M, P), (B, M)
        batch_size, n_input_points, dim_in = x.shape  # B, M, P

        # since scale is a per-caps scalar and we have one vote per capsule
        vote_component_pdf = self._get_pdf(self._votes,
                                           self._scales.unsqueeze(-1))

        # expand input along caps dimensions
        expanded_x = x.unsqueeze(1)  # (B, 1, M, P)
        vote_log_prob_per_dim = vote_component_pdf.log_prob(expanded_x)  # (B, O, M, P)
        vote_log_prob = vote_log_prob_per_dim.sum(-1)  # (B, O, M)

        # (B, 1, M)
        dummy_vote_log_prob = torch.zeros(
            batch_size, 1, n_input_points) - 2. * np.log(10.)

        # (B, O+1, M)
        vote_log_prob = torch.cat([vote_log_prob, dummy_vote_log_prob], 1)

        mixing_logits = math_ops.log_safe(self._vote_presence_prob)  # (B, O, M)

        dummy_logit = torch.zeros(batch_size, 1, 1) - 2. * np.log(10.)
        dummy_logit = dummy_logit.repeat(1, 1, n_input_points)  # (B, 1, M)

        # (B, O+1, M)
        mixing_logits = torch.cat([mixing_logits, dummy_logit], 1)
        mixing_log_prob = mixing_logits - mixing_logits.logsumexp(
            1, keepdim=True)  # (B, O+1, M)

        # (B, M)
        mixture_log_prob_per_point = (mixing_logits + vote_log_prob).logsumexp(1)

        if presence is not None:
            presence = presence.float()
            mixture_log_prob_per_point *= presence

        # (B,)
        mixture_log_prob_per_example = mixture_log_prob_per_point.sum(1)

        # scalar
        mixture_log_prob_per_batch = mixture_log_prob_per_example.mean()

        # (B, O + 1, M)
        posterior_mixing_logits_per_point = mixing_logits + vote_log_prob

        # [B, M]
        winning_vote_idx = torch.argmax(
            posterior_mixing_logits_per_point[:, :-1], 1)

        batch_idx = torch.arange(batch_size).unsqueeze(1)  # (B, 1)
        batch_idx = batch_idx.repeat(1, n_input_points)  # (B, M)

        point_idx = torch.arange(n_input_points).unsqueeze(0)  # (1, M)
        point_idx = point_idx.repeat(batch_size, 1)  # (B, M)

        idx = torch.stack([batch_idx, winning_vote_idx, point_idx], -1)

        # (B, M, P)
        winning_vote = self._votes[idx[:, :, 0], idx[:, :, 1], idx[:, :, 2]]
        assert winning_vote.shape == (batch_size, n_input_points, dim_in)

        # (B, M)
        winning_pres = \
            self._vote_presence_prob[idx[:, :, 0], idx[:, :, 1], idx[:, :, 2]]
        assert winning_pres.shape == (batch_size, n_input_points)

        # (B, O, M)
        vote_presence = mixing_logits[:, :-1] > mixing_logits[:, -1:]

        # the first four votes belong to the square
        is_from_capsule = winning_vote_idx // self._n_votes

        # (B, O+1, M)
        posterior_mixing_probs = F.softmax(posterior_mixing_logits_per_point, 1)

        dummy_vote = self.dummy_vote.repeat(batch_size, 1, 1, 1)  # (B, 1, M, P)
        dummy_pres = torch.zeros([batch_size, 1, n_input_points])

        votes = torch.cat((self._votes, dummy_vote), 1)  # (B, O+1, M, P)
        pres = torch.cat([self._vote_presence_prob, dummy_pres], 1)  # (B, O+1, M)

        # (B, M, P)
        soft_winner = torch.sum(posterior_mixing_probs.unsqueeze(-1) * votes, 1)
        assert soft_winner.shape == (batch_size, n_input_points, dim_in)

        # (B, M)
        soft_winner_pres = torch.sum(posterior_mixing_probs * pres, 1)
        assert soft_winner_pres.shape == (batch_size, n_input_points)

        # (B, M, O+1)
        posterior_mixing_probs = posterior_mixing_probs[:, :-1].transpose(1, 2)

        return self.Result(
            log_prob=mixture_log_prob_per_batch,
            vote_presence=vote_presence.float(),
            winner=winning_vote,
            winner_pres=winning_pres,
            soft_winner=soft_winner,
            soft_winner_pres=soft_winner_pres,
            posterior_mixing_probs=posterior_mixing_probs,
            mixing_log_prob=mixing_log_prob,
            mixing_logits=mixing_logits,
            is_from_capsule=is_from_capsule,
        )


def capsule_entropy(caps_presence_prob, k=1, **unused_kwargs):
    """Computes entropy in capsule activations."""
    del unused_kwargs

    # caps_presence_prob (B, O)

    within_prob = math_ops.normalize(caps_presence_prob, 1)  # (B, O)
    within_example = math_ops.cross_entropy_safe(within_prob,
                                                 within_prob * k)  # scalar

    total_caps_prob = torch.sum(caps_presence_prob, 0)  # (O, )
    between_prob = math_ops.normalize(total_caps_prob, 0)  # (O, )
    between_example = math_ops.cross_entropy_safe(between_prob,
                                                  between_prob * k)  # scalar
    return within_example, between_example


# kl(aggregated_prob||uniform)
def neg_capsule_kl(caps_presence_prob, **unused_kwargs):
    del unused_kwargs

    num_caps = int(caps_presence_prob.shape[-1])
    return capsule_entropy(caps_presence_prob, k=num_caps)


# l2(aggregated_prob - constant)
def capsule_l2_loss(caps_presence_prob,
                    num_classes: int,
                    within_example_constant=None,
                    **unused_kwargs):
    """Computes l2 penalty on capsule activations."""

    del unused_kwargs

    batch_size, num_caps = caps_presence_prob.shape  # B, O

    if within_example_constant is None:
        within_example_constant = float(num_caps) / num_classes

    between_example_constant = float(batch_size) / num_classes

    within_example = torch.mean(
        (caps_presence_prob.sum(1) - within_example_constant) ** 2)

    between_example = torch.mean(
        (caps_presence_prob.sum(0) - between_example_constant) ** 2)

    # neg between example because it's subtracted from the loss later on
    # TODO(BDS): remove negate on between_example
    return within_example, -between_example


def sparsity_loss(loss_type, *args, **kwargs):
    """Computes capsule sparsity loss according to the specified type."""
    if loss_type == 'entropy':
        sparsity_func = capsule_entropy
    elif loss_type == 'kl':
        sparsity_func = neg_capsule_kl
    elif loss_type == 'l2':
        sparsity_func = capsule_l2_loss
    else:
        raise ValueError(
            'Invalid sparsity loss: "{}"'.format(loss_type))

    return sparsity_func(*args, **kwargs)
